{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iCatcher annotations --> looking time pipeline\n",
    "\n",
    "This script, and the helper classes that it calls, are based on Raz et al., 2024 (https://osf.io/ndkt6/), and organizes iCatcher annotations and other subject and trial level data into a format that can feed into our R looking time pipeline. It does this by organizing subject, trial, and look level data in the same format as it is usually outputed in by Datavyu manual coding. \n",
    "\n",
    "## Inputs: \n",
    "\n",
    "### 1. Lookit study-level JSON file: \n",
    "\n",
    "We parse the Lookit study-wide .json log (that is stored at /data/metadata/lookit_study.json), to obtain trial onsets/offsets, trial data, subject data and session data, all in one go.\n",
    "    \n",
    "### 2. iCatcher annotation file/s (.npz file per subject, per session)\n",
    "\n",
    "These are the main outputs from running iCatcher. Expects one file per trial per subject, per session, named accordingly. \n",
    "\n",
    "### 3. raw video files (or video-parsed .json files)\n",
    "\n",
    "needed to extract frame rates for conversion from frame rates to ms (to get look events onset/offset relative to start of video)\n",
    "    \n",
    "if this conversion has already been run, .json files with the relevant information should exist in the videos directory. if not, these .json files will be written in the process of obtaining this info \n",
    "   \n",
    "Since we have a separate video for each trial we are not worried about calculating the exact trial onset time. However, we do store the lag between the onset of the audio and the video recording on each trial to use in our main looking time analysis in R. The original Raz et al., 2024 code (https://osf.io/ndkt6/) provides detailed instructions on how to deal with trial onsets relative to the start of videos, both when a manual onset CSV file is required and is not. The original code also provides flexibility with subject level and experiment onset information.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b668a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06fcfb",
   "metadata": {},
   "source": [
    "## import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e11725f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from helperfuncs.video_framerates import get_frame_information\n",
    "from helperfuncs.lookit_json_parser import get_lookit_trial_times\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e2d0e",
   "metadata": {},
   "source": [
    "## Set relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5bfe3c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data_dir = op.join(SERVER_PATH, 'data', 'raw')\n",
    "data_dir = op.join(PROJECT_PATH, 'data', PROJECT_VERSION)\n",
    "metadata_dir = op.join(PROJECT_PATH, 'data', 'metadata')\n",
    "\n",
    "# where the raw iCatcher outputs are stored\n",
    "icatcher_outputs_dir = op.join(raw_data_dir, 'icatcher_annotations')\n",
    "# where the raw videos are stored\n",
    "videos_dir = op.join(raw_data_dir, 'original_videos', 'mp4') \n",
    "\n",
    "# where the raw Lookit JSON is stored - thisonly raw data file that is stored locally \n",
    "lookit_json = op.join(PROJECT_PATH, 'data', 'raw', 'lookit','lookit_study.json')\n",
    "\n",
    "# where the trial timing info, for this specific project version, processed from the raw Lookit JSON is stored\n",
    "lookit_trial_info_csv = op.join(data_dir, 'data_to_analyze', 'lookit_trial_timing_info.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a332d3",
   "metadata": {},
   "source": [
    "### Create all necessary functions\n",
    "\n",
    "#### get all non-hidden files in dir (helper function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28c8679",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all files except those beginning with '.' i.e., hidden files \n",
    "def listdir_nohidden(path):\n",
    "    for root, dirs, files in os.walk(path):  # Walk through each directory and subdirectory\n",
    "        for f in files:\n",
    "            if not f.startswith('.') and f.endswith('.npz'):  # Check conditions\n",
    "                yield os.path.relpath(os.path.join(root, f), start=path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5d7d7",
   "metadata": {},
   "source": [
    "## Set parameters for manual error exclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7515a881",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include_manual_edits = 0\n",
    "\n",
    "if include_manual_edits:\n",
    "    manually_coded_sections = pd.read_csv(\"manual_coding_timestamps.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995eafb",
   "metadata": {},
   "source": [
    "#### convert iCatcher annotated look-events from frame-wise to timing (ms from video onset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95b3629",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsample_data(data, target_length):\n",
    "    \"\"\"\n",
    "    Subsample the input data to match the target length.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): The input data to subsample.\n",
    "        target_length (int): The desired length of the output data.\n",
    "    \n",
    "    Returns:\n",
    "        array-like: Subsampled data.\n",
    "    \"\"\"\n",
    "    factor = len(data) / target_length\n",
    "    indices = np.round(np.arange(0, len(data), factor)).astype(int)[:target_length]\n",
    "    return data[indices]\n",
    "\n",
    "def read_convert_output(filename, stamps,manual_edits_df=None):\n",
    "    \"\"\"\n",
    "    Given an npz file containing icatcher annotated frames and looks,\n",
    "    converts to pandas DataFrame with another column mapping each frame\n",
    "    to its time stamp in the video\n",
    "    \n",
    "    INPUTS: \n",
    "    filename (string): name of tabulated iCatcher output file in format\n",
    "    '[CHILD_ID]/[TRIAL_ID]_[CHILD_ID].npz'\n",
    "    stamps (List[int]): time stamp for each frame, where stamps[i] is the \n",
    "    time stamp at frame i (determined in function get_frame_information(), IMPORTED function)\n",
    "    \n",
    "    OUTPUTS: \n",
    "    rtype: DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    npz = np.load(filename)\n",
    "    df = pd.DataFrame([])\n",
    "    lst = npz.files\n",
    "    npz_data = npz[lst[0]]\n",
    "    confidence_data = npz[lst[1]]\n",
    "\n",
    "    # Align frame rates of iCatcher and mp4 videos, frame rates coming in are variable because of mp4 conversion\n",
    "    if len(npz_data) != len(stamps):\n",
    "        if len(npz_data) > len(stamps):\n",
    "            npz_data = subsample_data(npz_data, len(stamps))\n",
    "            confidence_data = subsample_data(confidence_data, len(stamps))\n",
    "        else:\n",
    "            stamps = subsample_data(stamps, len(npz_data))\n",
    "    \n",
    "    df['frame'] = range(1, len(npz_data) + 1)\n",
    "    df['lookType_coded'] = npz_data\n",
    "    # {'noface': -2, 'nobabyface': -1, 'away': 0, 'left': 1, 'right': 2}\n",
    "    df['lookType_coded'] = df['lookType_coded'].clip(lower=0)\n",
    "    # 'left' is coded as 1, 'right' as 2, we need to switch those since the video coming in from iCatcher+ is mirrored\n",
    "    df['lookType_coded'] = df['lookType_coded'].replace({1: 'right', 2: 'left', 0: 'away'})\n",
    "    \n",
    "    # convert frames to ms using frame rate\n",
    "    df['time_ms'] = stamps\n",
    "    df['time_ms'] = df['time_ms'].astype(int)\n",
    "    \n",
    "    df['confidence'] = confidence_data\n",
    "    \n",
    "    # SET left/right/away error FRAMES based on manual indexing, from CSV  \n",
    "    if include_manual_edits: \n",
    "        if len(manual_edits_df):\n",
    "            for index, row in manual_edits_df.iterrows():\n",
    "                onset = row['onset']\n",
    "                offset = row['offset']\n",
    "                df.loc[onset:offset, 'lookType_coded'] = row['value']\n",
    "                df.loc[onset:offset, 'confidence'] = -1\n",
    "\n",
    "    # split into dfs based on when the change happens\n",
    "    df['group'] = df['lookType_coded'].ne(df['lookType_coded'].shift()).cumsum()\n",
    "    df_grps = df.groupby('group')\n",
    "    \n",
    "    dfs = []\n",
    "    for _, data in df_grps:\n",
    "        dfs.append(data)\n",
    "\n",
    "\n",
    "    looks_onoff_grouped = pd.DataFrame()\n",
    "    \n",
    "    for grpI in range(len(dfs)):\n",
    "        indices = dfs[grpI].index.tolist()\n",
    "        # first row time_ms is onset, row after last row time_ms is offset \n",
    "        onset = df.iloc[indices[0]].time_ms\n",
    "\n",
    "        if grpI == len(dfs)-1:\n",
    "            # assume dur of last frame is the mean dur of frames \n",
    "            dur = np.floor(np.mean(np.diff(df['time_ms'])))\n",
    "            offset = onset + dur\n",
    "        else:\n",
    "            offset = df.iloc[indices[-1]+1].time_ms\n",
    "        \n",
    "        lookType = df.iloc[indices[0]].lookType_coded\n",
    "        confidence = np.mean(df.iloc[indices].confidence)\n",
    "    \n",
    "        cur_lookinfo = pd.DataFrame({\"Looks.ordinal\": grpI,\n",
    "                    \"Looks.onset\" : onset,\n",
    "                    \"Looks.offset\": offset, \n",
    "                    \"Looks.lookType\": lookType,\n",
    "                    \"Looks.confidence\": confidence},  index=[0])\n",
    "\n",
    "        looks_onoff_grouped = pd.concat([looks_onoff_grouped, cur_lookinfo], ignore_index=True)\n",
    "\n",
    "\n",
    "    \n",
    "    return [looks_onoff_grouped, df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e72fe9b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lookit_info(child_id, trial_id, session_id, trial_info_file, lookit_json):\n",
    "    \n",
    "    if Path(lookit_trial_info_csv).is_file(): # check whether lookit file already parsed\n",
    "        df = pd.read_csv(trial_info_file)\n",
    "            \n",
    "    else: # otherwise, parse and save out relevant info  \n",
    "        os.makedirs(os.path.dirname(lookit_trial_info_csv), exist_ok=True)\n",
    "        df = get_lookit_trial_times(lookit_json)\n",
    "        df.to_csv(lookit_trial_info_csv)\n",
    "        \n",
    "    \n",
    "    # get part of df from current child\n",
    "    df = df[df['SubjectInfo.subjID'] == child_id] \n",
    "\n",
    "    # get part of df from current trial\n",
    "    df = df[df['Trials.trialID'] == trial_id]\n",
    "\n",
    "    if 'SubjectInfo.sessionNumber' in df:\n",
    "        df = df[df['SubjectInfo.sessionNumber'] == session_id] \n",
    "    else:\n",
    "        df['SubjectInfo.sessionNumber'] = 1\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119aa937",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lookit_trial_times(lookit_json=lookit_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5372dd0c",
   "metadata": {},
   "source": [
    "#### get trial onsets w/r/t video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e7b161",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trial_sets(child_id, trial_id, session_id, trial_info_file):\n",
    "    \"\"\"\n",
    "    Finds corresponding trial info \n",
    "    and returns a list of [onset, offset] times for each trial in \n",
    "    milliseconds, with respect to video onset\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    lookit_df = get_lookit_info(child_id, trial_id, session_id, trial_info_file, lookit_json)\n",
    "    df = lookit_df[[col for col in lookit_df.columns if col.startswith('Trials.')]]    \n",
    "    df = df.copy()\n",
    "    df['Trials.ordinal'] = df.index.values.tolist()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d3e7d",
   "metadata": {},
   "source": [
    "#### Get subject level data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "950dc587",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subject_info(child_id, trial_id, session_id, trial_info_file): \n",
    "        \n",
    "    lookit_df = get_lookit_info(child_id, trial_id, session_id, trial_info_file, lookit_json)\n",
    "    df = lookit_df[[col for col in lookit_df.columns if col.startswith('SubjectInfo.')]]       \n",
    "    if 'SubjectInfo.sessionNumber' in df:\n",
    "        df = df[df['SubjectInfo.sessionNumber'] == session_id] \n",
    "    else:\n",
    "        df['SubjectInfo.sessionNumber'] = 1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SERVER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57323a02",
   "metadata": {},
   "source": [
    "### main function: run processes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faeb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up output name, saving a single file for each session of the experiment\n",
    "output_data_dir = op.join(PROJECT_PATH, 'data', PROJECT_VERSION, 'data_to_analyze')\n",
    "fname_final_output = op.join(output_data_dir, f'processed_icatcher.csv')\n",
    "# create a list of participants that have already been processed for efficiency\n",
    "if op.exists(fname_final_output):\n",
    "    existing_df = pd.read_csv(fname_final_output)\n",
    "    processed_participants = list(zip(existing_df['SubjectInfo.subjID'], existing_df['Trials.trialID']))\n",
    "else:\n",
    "    processed_participants = []\n",
    "\n",
    "if not any(listdir_nohidden(icatcher_outputs_dir)):\n",
    "    print(\"No iCatcher files found, make sure you are connected to your server\")\n",
    "for filename in listdir_nohidden(icatcher_outputs_dir):\n",
    "    # Only include files that are placed in folders set up in a directory for a particular child\n",
    "    if '/' in filename and ('easy' in filename or 'hard' in filename):\n",
    "        # Split into child_id and trial id\n",
    "        child_id, trial_id_with_extension_child_id = filename.split('/', 1)\n",
    "        # Remove the .npz extension from trial id\n",
    "        trial_id_with_child_id = trial_id_with_extension_child_id.rsplit('.', 1)[0]  \n",
    "        trial_id = trial_id_with_child_id.split(\"_\")[0]\n",
    "    else:\n",
    "        continue\n",
    "    print('child: ', child_id)\n",
    "    print('trial: ', trial_id)\n",
    "    # Need to pull session information in the future, our current experiment only uses a single session.\n",
    "    session_id = 1\n",
    "    # Check if this subject's data already exists in output file\n",
    "    if (child_id, trial_id) in processed_participants:\n",
    "        print(f'Skipping {child_id}, {trial_id} - already exists in output file')\n",
    "        continue\n",
    "    # determine trial info files \n",
    "    trial_info_file = lookit_trial_info_csv\n",
    "\n",
    "    # get trial onsets and offsets from input file, match to iCatcher file\n",
    "    trials_df = get_trial_sets(child_id, trial_id, session_id, trial_info_file)\n",
    "    trials_df = trials_df.reset_index(drop=True)\n",
    "    if (trials_df.empty):\n",
    "        print(f'Skipping {child_id}, {trial_id} - no trial info found')\n",
    "        continue\n",
    "    \n",
    "    if include_manual_edits:\n",
    "        manual_edits_df = manually_coded_sections[manually_coded_sections['child_id'] == child_id and manually_coded_sections['trial_id'] == trial_id]\n",
    "    \n",
    "    # determine video source    \n",
    "    vid_path = op.join(videos_dir, child_id, f\"{trial_id}_{child_id}.mp4\")\n",
    "    json_video_data = op.join(videos_dir, child_id, f\"{trial_id}_{child_id}.json\")\n",
    "    # get timestamp for each frame in the video\n",
    "    timestamps, length = get_frame_information(vid_path, json_video_data)\n",
    "    if not timestamps:\n",
    "        print('video not found for {} in {} folder'.format(child_id, videos_dir))\n",
    "        continue\n",
    "\n",
    "    # initialize df with time stamps for iCatcher file\n",
    "    icatcher_path = icatcher_outputs_dir + '/' + filename\n",
    "    \n",
    "    if include_manual_edits:\n",
    "        [looks_df_grouped, looks_df] = read_convert_output(icatcher_path, timestamps, manual_edits_df)\n",
    "    else:\n",
    "        [looks_df_grouped, looks_df] = read_convert_output(icatcher_path, timestamps)\n",
    "    \n",
    "    looks_df_grouped = looks_df_grouped[['Looks.ordinal', 'Looks.onset', 'Looks.offset', 'Looks.lookType', 'Looks.confidence']]\n",
    "    looks_df_grouped = looks_df_grouped.reset_index(drop=True)\n",
    "\n",
    "    # make subject level dataframe\n",
    "    subject_info = get_subject_info(child_id, trial_id, session_id, trial_info_file)\n",
    "    subject_info = subject_info.loc[[0], :].copy()\n",
    "    subject_info['SubjectInfo.subjID'] = subject_info['SubjectInfo.subjID'].astype(str)\n",
    "    subject_info = subject_info.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    df_concat = pd.concat([pd.DataFrame(np.repeat(subject_info.values, len(looks_df_grouped), axis=0), columns=subject_info.columns), \n",
    "                           looks_df_grouped, pd.DataFrame(np.repeat(trials_df.values, len(looks_df_grouped), axis=0), columns=trials_df.columns)], axis=1)\n",
    "    \n",
    "    # add trial_id and child_id to the dataframe\n",
    "    looks_df['Trials.trialID'] = trial_id\n",
    "    looks_df['SubjectInfo.subjID'] = child_id\n",
    "\n",
    "    if not os.path.exists(output_data_dir):\n",
    "        os.makedirs(output_data_dir)\n",
    "    # Append or create files for df_concat\n",
    "    if os.path.exists(fname_final_output):\n",
    "        looks_df.to_csv(fname_final_output, mode='a', header=False, index=False)  # Append without header\n",
    "    else:\n",
    "        looks_df.to_csv(fname_final_output, index=False)  # Create new file        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "324425a09df9827eca06f7fdcdada0d9b56b15dce66bd3915a40fe7c652fd319"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
