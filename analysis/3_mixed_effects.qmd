```{r message=FALSE}
library(tidyverse)
library(here)
library(lmerTest)
library(MuMIn)
library(lme4)
library(dotenv)
library(broom)
library(broom.mixed)
library(effects)
library(emmeans)


load_dot_env(file=here(".env"))
PROJECT_VERSION = Sys.getenv("PROJECT_VERSION")
source("lmer_helpers.R")
```

### Load data
```{r}
trial_metadata <- read.csv(here("data","metadata","level-trialtype_data.csv"))
trial_summary_data <- read.csv(here("data",PROJECT_VERSION, "processed_data","level-trials_data.csv"))

usable_trials <- trial_summary_data |>
  filter(exclude_participant_insufficient_data == 0 & trial_exclusion == 0 & exclude_participant == 0) 

# Merging with similarity information and mean-centering main effects
trials_with_effect_vars <- usable_trials |>
  left_join(trial_metadata) |>
  mutate(age_in_months = SubjectInfo.testAge/30)
```
Sanity check - making sure all participants have at least 16 trials and that we have 79 participants
```{r}
trials_with_effect_vars |> distinct(SubjectInfo.subjID,Trials.trialID) |> summarize(n=n(),.by=SubjectInfo.subjID) |> filter(n < 20)
nrow(trials_with_effect_vars |> distinct(SubjectInfo.subjID))
```

### Run mixed-effects model
Model 1: This is the model we said we'd run in our pre-reg
```{r}
prereg_main_effect <- lmer(scale(corrected_target_looking) ~ scale(text_similarity)*scale(age_in_months)
                    + (scale(text_similarity) | SubjectInfo.subjID)
                    + (1|Trials.targetImage) 
                    + (1|Trials.imagePair), 
                    data = trials_with_effect_vars)


summary(prereg_main_effect)
```
Currently, it looks like text_similarity is on the edge of significance. Let me estimate using tidy()
```{r}
tidy_model <- function(main_effect){
  table_data <- tidy(main_effect, effects = "fixed") %>%
  mutate(
    p.value = 2 * (1 - pnorm(abs(statistic))),  # Calculate p-values for lmer
    p.value.condensed = case_when(
      p.value < .001 ~ "<.001", 
      p.value < .01 ~ "<.01",
      p.value < .05 ~ "<.05",
      TRUE ~ sprintf("%.3f", p.value)),
    term = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "scale(age_in_months)" ~ "Age (scaled)",
      #term == "scale(image_similarity)" ~ "Target-distractor image embedding similarity (scaled)",
      TRUE ~ term
    )
  ) %>%
  rename(
    Predictor = term,
    "b" = estimate,
    "SE" = std.error,
    "t" = statistic,  # Note: changed from z to t for lmer
    "p" = p.value.condensed,
    "p.full" = p.value
  ) %>%
  mutate(across(c("b", "SE", "t"), ~round(., 2)))
  return(table_data)
}
prereg_model <- tidy_model(prereg_main_effect)
prereg_model
```
Now it is significant..

Swapping text similarity with image similarity:
```{r}
image_sim_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)
                    + (scale(image_similarity) | SubjectInfo.subjID)
                    + (1|Trials.targetImage) 
                    + (1|Trials.imagePair), 
                    data = trials_with_effect_vars)
```
running into a singular fit, I tried removing each of the random effects but only removing the random slope of image_similarity fixed this
```{r}
image_sim_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)
                    + (1|SubjectInfo.subjID)
                    + (1|Trials.targetImage) 
                    + (1|Trials.imagePair), 
                    data = trials_with_effect_vars)
summary(image_sim_effect)
```
Let's try removing that random slope from the text model for consistency
```{r}
text_sim_effect_short <- lmer(scale(corrected_target_looking) ~ scale(text_similarity)*scale(age_in_months)
                    + (1|SubjectInfo.subjID)
                    + (1|Trials.targetImage) 
                    + (1|Trials.imagePair), 
                    data = trials_with_effect_vars)
summary(text_sim_effect_short)
```
Still comparable to the old model, looking back up, similarity and image pair explained very little variance to begin with so maybe this is ok.

Now for the fun stuff: adding in our covariates
```{r}
main_image_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage)
                    + (1|Trials.imagePair), 
                    data = trials_with_effect_vars)
```
Singular again! Getting rid of image pair random effect.

```{r}
main_image_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

main_text_effect <- lmer(scale(corrected_target_looking) ~ scale(text_similarity)*scale(age_in_months)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)
summary(main_image_effect)
summary(main_text_effect)
r.squaredGLMM(main_text_effect)
```
Only the text similarity effect is significant which is interesting.

```{r}
image_model <- tidy_model(main_image_effect)
image_model
```

```{r}
vs_image_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

vs_text_effect <- lmer(scale(corrected_target_looking) ~ scale(text_similarity)*scale(age_in_months)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)
summary(vs_image_effect)
summary(vs_text_effect)
```
Just a sanity check that adding our saliency metric as a covariate does not affect our similarity effects.

Checking if text similarity and image similarity are differently correlated with AoA
```{r}
cor.test(trial_metadata$AoA_Est_target, trial_metadata$image_similarity)
cor.test(trial_metadata$AoA_Est_target, trial_metadata$text_similarity)
```
Well insignificant p-values but there is a higher R^2 with image similarity.

### Alternate window analyses

Basline window as a covariate
```{r}
baseline_covariate_looking_text <- lmer(scale(mean_target_looking_critical_window) ~ scale(age_in_months)*scale(text_similarity)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + scale(mean_target_looking_baseline_window)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

baseline_covariate_looking_image <- lmer(scale(mean_target_looking_critical_window) ~ scale(age_in_months)*scale(image_similarity)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + scale(mean_target_looking_baseline_window)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

summary(baseline_covariate_looking_text)
summary(baseline_covariate_looking_image)
```
Similar predictions to our original model.

Adding window type as a covariate
```{r}
trials_window_type_separated <- trials_with_effect_vars |>
  pivot_longer(cols=c(mean_target_looking_critical_window, mean_target_looking_baseline_window), names_to="window_type", values_to="target_looking") |>
  mutate(window_type = str_replace(window_type, "mean_target_looking_", "")) |>
  mutate(trial_window_c = case_when(
    window_type=="critical_window" ~ 0.5,
    window_type=="baseline_window" ~ -0.5))

window_type_looking_text <- lmer(scale(target_looking) ~ scale(age_in_months)*trial_window_c*scale(text_similarity)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_window_type_separated)

window_type_looking_image <- lmer(scale(target_looking) ~ scale(age_in_months)*trial_window_c*scale(image_similarity)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_window_type_separated)

summary(window_type_looking_text)
summary(window_type_looking_image)
```
The interaction makes sense, plotting the predictions here to try to understand what's going on
```{r}
trials_window_type_separated$predicted <- predict(window_type_looking_image)

# Plot interaction effect
ggplot(trials_window_type_separated, aes(x = image_similarity, y = predicted, color = factor(window_type))) +
  geom_point(alpha = 0.5) +   # Add points for raw data
  geom_smooth(method = "lm", se = TRUE) +  # Add regression lines
  labs(title = "Interaction Between Trial Window & Image Similarity",
       x = "Scaled Image Similarity",
       y = "Predicted Target Looking",
       color = "Trial Window") +
  theme_minimal()
```

Using a shorter critical window
```{r}
short_image_effect <- lmer(scale(corrected_target_looking_short) ~ scale(image_similarity)*scale(age_in_months)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

short_text_effect <- lmer(scale(corrected_target_looking_short) ~ scale(text_similarity)*scale(age_in_months)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)
summary(short_image_effect)
summary(short_text_effect)
r.squaredGLMM(main_text_effect)
```
Well age is significant and seems to soak up the similarity effects we found. Interesting that younger infants seem to take longer to recognize our target words and will be interesting to explore if there is any interaction between reaction time and similarity, especially with younger infants as a next step.

