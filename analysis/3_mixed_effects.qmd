```{r message=FALSE}
library(tidyverse)
library(here)
library(lmerTest)
library(MuMIn)
library(lme4)
library(dotenv)
library(broom)
library(broom.mixed)
library(effects)
library(emmeans)

env_file = here(".env")
default_project = "main"
if (file.exists(env_file)) {
  load_dot_env(file = env_file)
  PROJECT_VERSION <- Sys.getenv("PROJECT_VERSION")
  if (PROJECT_VERSION == "") {
    PROJECT_VERSION <- default_project
  }
} else {
  PROJECT_VERSION <- default_project
}
source("lmer_helpers.R")
```

# Load data
```{r}
trial_metadata <- read.csv(here("data","metadata","level-trialtype_data.csv"))
trial_summary_data <- read.csv(here("data",PROJECT_VERSION, "processed_data","level-trials_data.csv"))

usable_trials <- trial_summary_data |>
  filter(exclude_participant_insufficient_data == 0 & trial_exclusion == 0 & exclude_participant == 0) 

# Merging with similarity information and mean-centering main effects
trials_with_effect_vars <- usable_trials |>
  left_join(trial_metadata) |>
  mutate(age_in_months = SubjectInfo.testAge/30)
```
Sanity check - making sure all participants have at least 16 trials and that we have 83 participants
```{r}
low_trial_count <- trials_with_effect_vars |> distinct(SubjectInfo.subjID,Trials.trialID) |> summarize(n=n(),.by=SubjectInfo.subjID) |> filter(n < 25)
nrow(trials_with_effect_vars |> distinct(SubjectInfo.subjID))
```

# Run mixed-effects model
## Model 1: This is the model we said we'd run in our pre-reg
```{r}
prereg_main_effect <- lmer(scale(corrected_target_looking) ~ scale(text_similarity)*scale(age_in_months)
                    + (scale(text_similarity) | SubjectInfo.subjID)
                    + (1|Trials.targetImage) 
                    + (1|Trials.imagePair),
                    data = trials_with_effect_vars)

summary(prereg_main_effect)
```
Currently, it looks like text_similarity is on the edge of significance. Let me estimate using tidy()
```{r}
tidy_model <- function(main_effect){
  table_data <- tidy(main_effect, effects = "fixed") %>%
  mutate(
    #p.value = 2 * (1 - pt(abs(statistic),df)),  # Calculate p-values for lmer - just using default calculated ones
    p.value.condensed = case_when(
      p.value < .001 ~ "<.001", 
      p.value < .01 ~ "<.01",
      p.value < .05 ~ "<.05",
      TRUE ~ sprintf("%.3f", p.value)),
    term = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "scale(age_in_months)" ~ "Age (scaled)",
      #term == "scale(image_similarity)" ~ "Target-distractor image embedding similarity (scaled)",
      TRUE ~ term
    )
  ) %>%
  rename(
    Predictor = term,
    "b" = estimate,
    "SE" = std.error,
    "t" = statistic,  # Note: changed from z to t for lmer
    "p" = p.value.condensed,
    "p.full" = p.value
  ) %>%
  mutate(across(c("b", "SE", "t"), ~round(., 2)))
  return(table_data)
}
prereg_model <- tidy_model(prereg_main_effect)
```

## Singular fit debugs for image similarity
Swapping text similarity with image similarity:
```{r}
image_sim_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)
                    + (scale(image_similarity) || SubjectInfo.subjID)
                    + (1|Trials.targetImage) 
                    + (1|Trials.imagePair), 
                    data = trials_with_effect_vars)
summary(image_sim_effect)
```
running into a singular fit, I tried removing each of the random effects but only removing the random slope of image_similarity and the random intercept for image pair fixed this. This is okay for now since we see the same effect with the singular model and this model.
```{r}
image_sim_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)
                     + (1 | SubjectInfo.subjID)
                     + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)
summary(image_sim_effect)
```


## Adding in covariates
Now for the fun stuff: adding in our covariates using our original model -- only doing this for text similarity for now because of the singular effects
```{r}
main_image_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)*scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

main_text_effect <- lmer(scale(corrected_target_looking) ~ scale(text_similarity)*scale(age_in_months)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (scale(text_similarity) | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)
summary(main_image_effect)
summary(main_text_effect)
r.squaredGLMM(main_text_effect)
```
Text similarity is still on the verge of significance

Checking if text similarity and image similarity are differently correlated with AoA
```{r}
cor.test(trial_metadata$AoA_Est_target, trial_metadata$text_similarity)
cor.test(trial_metadata$AoA_Est_target, trial_metadata$image_similarity)
```
Well image similarity has a higher r but both are still insignificant.


```{r}
image_model <- summary(main_image_effect)
image_model
```

```{r}
vs_image_effect <- lmer(scale(corrected_target_looking) ~ scale(image_similarity)*scale(age_in_months)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

vs_text_effect <- lmer(scale(corrected_target_looking) ~ scale(text_similarity)*scale(age_in_months)
                    + scale(MeanSaliencyDiff)
                    + (scale(text_similarity) | SubjectInfo.subjID) 
                    + (1|Trials.targetImage)
                    + (1|Trials.imagePair), 
                    data = trials_with_effect_vars)
summary(vs_image_effect)
summary(vs_text_effect)
```
Just a sanity check that adding our saliency metric as a covariate does not affect our similarity effects.


# Alternate window analyses

## Baseline window as a covariate
```{r}
baseline_covariate_looking_text <- lmer(scale(mean_target_looking_critical_window) ~ scale(age_in_months)*scale(text_similarity)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + scale(mean_target_looking_baseline_window)
                    + (scale(text_similarity) | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

baseline_covariate_looking_image <- lmer(scale(mean_target_looking_critical_window) ~ scale(age_in_months)*scale(image_similarity)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + scale(mean_target_looking_baseline_window)
                    + (scale(image_similarity)| SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

tidy_model(baseline_covariate_looking_text)
tidy_model(baseline_covariate_looking_image)
```
Similar predictions to our original model.

## Adding window type as a covariate
```{r}
trials_window_type_separated <- trials_with_effect_vars |>
  pivot_longer(cols=c(mean_target_looking_critical_window, mean_target_looking_baseline_window), names_to="window_type", values_to="target_looking") |>
  mutate(window_type = str_replace(window_type, "mean_target_looking_", "")) |>
  mutate(trial_window_c = case_when(
    window_type=="critical_window" ~ 0.5,
    window_type=="baseline_window" ~ -0.5))

window_type_looking_text <- lmer(scale(target_looking) ~ scale(age_in_months)*trial_window_c*scale(text_similarity)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (scale(text_similarity) | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_window_type_separated)

window_type_looking_image <- lmer(scale(target_looking) ~ scale(age_in_months)*trial_window_c*scale(image_similarity)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (scale(image_similarity) | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_window_type_separated)

tidy_model(window_type_looking_text)
tidy_model(window_type_looking_image)
```

The interaction makes sense, plotting the predictions here to try to understand what's going on
```{r}
trials_window_type_separated$predicted <- predict(window_type_looking_image)

# Plot interaction effect
ggplot(trials_window_type_separated, aes(x = image_similarity, y = predicted, color = factor(window_type))) +
  geom_point(alpha = 0.5) +   # Add points for raw data
  geom_smooth(method = "lm", se = TRUE) +  # Add regression lines
  labs(title = "Interaction Between Trial Window & Image Similarity",
       x = "Scaled Image Similarity",
       y = "Predicted Target Looking",
       color = "Trial Window") +
  theme_minimal()
```

## Using a shorter critical window
```{r}
short_image_effect <- lmer(scale(corrected_target_looking_short) ~ scale(image_similarity)*scale(age_in_months)
                    + scale(AoA_Est_target)
                    + scale(MeanSaliencyDiff)
                    + (1 | SubjectInfo.subjID) 
                    + (1|Trials.targetImage), 
                    data = trials_with_effect_vars)

short_text_effect <- lmer(scale(corrected_target_looking_short) ~ scale(text_similarity)*scale(age_in_months)
                    + (scale(text_similarity) || SubjectInfo.subjID)
                    + (1 | Trials.targetImage), 
                    data = trials_with_effect_vars)
summary(short_text_effect)
```
Text similarity model does not converge with a shorter critical window.


